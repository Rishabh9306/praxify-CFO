{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic CFO Copilot: AIML Engine Demonstration\n",
    "\n",
    "This notebook provides a complete, end-to-end demonstration of the AIML engine. We will walk through each core module, from ingesting a raw CSV file to generating a final, dashboard-ready JSON output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup and Imports\n",
    "\n",
    "First, let's import all the necessary modules from our `aiml_engine` and other libraries. Make sure you have installed all dependencies from `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aiml_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Add the project root to the Python path to allow for module imports\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maiml_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_ingestion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataIngestion\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maiml_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataValidationQualityAssuranceEngine\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maiml_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_engineering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KPIAutoExtractionDynamicFeatureEngineering\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aiml_engine'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Add the project root to the Python path to allow for module imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from aiml_engine.core.data_ingestion import DataIngestion\n",
    "from aiml_engine.core.data_validation import DataValidationQualityAssuranceEngine\n",
    "from aiml_engine.core.feature_engineering import KPIAutoExtractionDynamicFeatureEngineering\n",
    "from aiml_engine.core.forecasting import ForecastingModule\n",
    "from aiml_engine.core.anomaly_detection import AnomalyDetectionModule\n",
    "from aiml_engine.core.correlation import CrossMetricCorrelationTrendMiningEngine\n",
    "from aiml_engine.core.simulation import ScenarioSimulationEngine\n",
    "from aiml_engine.core.dashboard import BusinessDashboardOutputLayer\n",
    "from aiml_engine.utils.helpers import serialize_to_json\n",
    "\n",
    "# Define the path to our sample data\n",
    "DATA_FILE_PATH = '../data/sample_financial_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Ingestion and Normalization\n",
    "\n",
    "We start by feeding a raw CSV file to the `DataIngestion` module. This module will automatically detect the column meanings (e.g., mapping `Sales Revenue` to `revenue`) and create a standardized DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading CSV file: [Errno 2] No such file or directory: '../data/sample_financial_data.csv'\n",
      "--- Header Mappings ---\n",
      "{}\n",
      "\n",
      "--- Normalized DataFrame Head ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ingestion_module = DataIngestion()\n",
    "normalized_df, header_mappings = ingestion_module.ingest_and_normalize(DATA_FILE_PATH)\n",
    "\n",
    "print(\"--- Header Mappings ---\")\n",
    "print(json.dumps(header_mappings, indent=2))\n",
    "print(\"\\n--- Normalized DataFrame Head ---\")\n",
    "display(normalized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Validation and Quality Assurance\n",
    "\n",
    "Next, we pass the normalized data through the validation engine. It will handle missing values, coerce data types, and produce a `validation_report` and a `corrections_log` detailing every change made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_module = DataValidationQualityAssuranceEngine()\n",
    "validated_df, validation_report, corrections_log = validation_module.run_pipeline(normalized_df, header_mappings)\n",
    "\n",
    "print(\"--- Validation Report ---\")\n",
    "print(json.dumps(validation_report, indent=2))\n",
    "print(\"\\n--- Corrections Log (Sample) ---\")\n",
    "print(json.dumps(corrections_log[:3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: KPI Auto-Extraction & Feature Engineering\n",
    "\n",
    "Now, we derive important financial KPIs and features like `profit_margin` and MoM growth rates. The `feature_schema` documents the origin and transformation of each new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_module = KPIAutoExtractionDynamicFeatureEngineering()\n",
    "featured_df, feature_schema = feature_module.extract_and_derive_features(validated_df)\n",
    "\n",
    "print(\"--- Feature Schema ---\")\n",
    "print(json.dumps(feature_schema, indent=2))\n",
    "print(\"\\n--- DataFrame with New Features (Head) ---\")\n",
    "display(featured_df[['date', 'revenue', 'expenses', 'profit', 'profit_margin', 'revenue_mom_growth']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Predictive Forecasting\n",
    "\n",
    "The forecasting module automatically selects the best model (between AutoARIMA and Prophet) and generates a 3-month forecast for the specified metric, including confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_module = ForecastingModule(metric='revenue', date_col='date')\n",
    "forecast, model_health = forecasting_module.generate_forecast(featured_df)\n",
    "\n",
    "print(\"--- Model Health Report ---\")\n",
    "print(json.dumps(model_health, indent=2))\n",
    "print(\"\\n--- Forecast Results ---\")\n",
    "print(json.dumps(forecast, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Forecast\n",
    "\n",
    "Let's plot the historical data along with the forecast to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if forecast:\n",
    "    forecast_df = pd.DataFrame(forecast)\n",
    "    forecast_df['date'] = pd.to_datetime(forecast_df['date'])\n",
    "\n",
    "    fig = go.Figure()\n",
    "    # Historical Data\n",
    "    fig.add_trace(go.Scatter(x=featured_df['date'], y=featured_df['revenue'], mode='lines', name='Historical Revenue'))\n",
    "    # Forecast Data\n",
    "    fig.add_trace(go.Scatter(x=forecast_df['date'], y=forecast_df['predicted'], mode='lines', name='Forecasted Revenue', line=dict(dash='dash')))\n",
    "    # Confidence Interval\n",
    "    fig.add_trace(go.Scatter(x=forecast_df['date'], y=forecast_df['upper'], fill=None, mode='lines', line_color='rgba(0,0,0,0)', name='Upper CI'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_df['date'], y=forecast_df['lower'], fill='tonexty', mode='lines', line_color='rgba(0,0,0,0)', name='Lower CI'))\n",
    "\n",
    "    fig.update_layout(title='Revenue Forecast', xaxis_title='Date', yaxis_title='Revenue')\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No forecast generated. Skipping visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Anomaly Detection\n",
    "\n",
    "The `AnomalyDetectionModule` scans the data for significant outliers and reports them with severity and a reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_module = AnomalyDetectionModule()\n",
    "anomalies = anomaly_module.detect_anomalies(featured_df, metric='revenue')\n",
    "\n",
    "print(\"--- Detected Anomalies ---\")\n",
    "print(json.dumps(anomalies, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Cross-Metric Correlation & Trend Mining\n",
    "\n",
    "Here, we uncover hidden relationships between different financial metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_module = CrossMetricCorrelationTrendMiningEngine()\n",
    "correlation_report = correlation_module.generate_correlation_report(featured_df)\n",
    "\n",
    "print(\"--- Correlation Report ---\")\n",
    "print(json.dumps(correlation_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Scenario Simulation (What-If Engine)\n",
    "\n",
    "Let's simulate the impact of a 15% increase in expenses on profit and cashflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_module = ScenarioSimulationEngine()\n",
    "simulation_results = simulation_module.simulate_scenario(df=featured_df, parameter='expenses', change_pct=15.0)\n",
    "\n",
    "print(\"--- Simulation Results ---\")\n",
    "print(serialize_to_json(simulation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Final Dashboard Generation\n",
    "\n",
    "Finally, we bring everything together into a single, cohesive JSON object ready to be consumed by a frontend dashboard. We will generate it in both 'Finance Guardian' and 'Financial Storyteller' modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_module = BusinessDashboardOutputLayer()\n",
    "\n",
    "# --- Finance Guardian Mode --- #\n",
    "guardian_output = dashboard_module.generate_dashboard(\n",
    "    featured_df=featured_df,\n",
    "    forecast=forecast,\n",
    "    anomalies=anomalies,\n",
    "    mode=\"finance_guardian\",\n",
    "    correlation_report=correlation_report,\n",
    "    simulation_results=simulation_results\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"          FINANCE GUARDIAN OUTPUT         \")\n",
    "print(\"=\"*50)\n",
    "print(serialize_to_json(guardian_output))\n",
    "\n",
    "# --- Financial Storyteller Mode --- #\n",
    "storyteller_output = dashboard_module.generate_dashboard(\n",
    "    featured_df=featured_df,\n",
    "    forecast=forecast,\n",
    "    anomalies=anomalies,\n",
    "    mode=\"financial_storyteller\",\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"        FINANCIAL STORYTELLER OUTPUT      \")\n",
    "print(\"=\"*50)\n",
    "print(serialize_to_json(storyteller_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
